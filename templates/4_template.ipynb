{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ongoing-brazil",
   "metadata": {},
   "source": [
    "# Preprocessing danych"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "celtic-uncle",
   "metadata": {},
   "source": [
    "Zacznijmy do zaimportowania podstawowych modułów i bibliotek. Upewnij się, że masz zainstalowany ```scikit-learn```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "composite-roller",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "approved-marathon",
   "metadata": {},
   "source": [
    "Następnie, pobieramy zbiór ze słynnego konkursu \"Titanic - Machine Learning from Disaster\" z poniższego linku. <br/>\n",
    "https://www.kaggle.com/c/titanic/data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "placed-irrigation",
   "metadata": {},
   "source": [
    "Kaggle to ważna strona w świecie data science i machine learning'u. Można na niej znaleźć masę zbiorów danych, praktyczne mikrokursy, notebooki i to z czego słynie najbardziej - competitions(w tym przyszłe lokalne BIT AI ;) ). Jeśli jeszcze tego nie zrobiłeś/aś, gorąco zachęcam do założenia konta."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expanded-missile",
   "metadata": {},
   "source": [
    "Dane wypakowujemy do wybranego folderu, a następnie wczytujemy je do data frame'ów. Poniższy kod zakłada, że pliki są w tym samym miejscu co ten notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imported-track",
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath = '.' #change accordingly\n",
    "\n",
    "def load_data(filename):\n",
    "    csv_path = os.path.join(datapath, filename)\n",
    "    return pd.read_csv(csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dominant-anthropology",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "train_data = load_data('train.csv')\n",
    "test_data = load_data('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "white-score",
   "metadata": {},
   "outputs": [],
   "source": [
    "?pd.read_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "excessive-nirvana",
   "metadata": {},
   "outputs": [],
   "source": [
    "??pd.read_csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "subsequent-failure",
   "metadata": {},
   "source": [
    "## Podstawowa analiza danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afraid-humor",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "literary-assignment",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "biological-reflection",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "treated-linux",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "train_data.hist(figsize=(8,8))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "structured-operator",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train_data = train_data.select_dtypes(exclude=['object'])\n",
    "cat_train_X = train_data.select_dtypes(include=['object'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "medical-quest",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train_X, y = num_train_data.drop('Survived', axis=1), num_train_data['Survived']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "changing-villa",
   "metadata": {},
   "source": [
    "## Eliminacja \"nieużytecznych\" zmiennych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tribal-thickness",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = train_data.corr()\n",
    "corr_matrix['Survived'].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adaptive-yeast",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train_X = num_train_X.drop('PassengerId', axis=1)\n",
    "cat_train_X = cat_train_X.drop('Name', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prescription-fitness",
   "metadata": {},
   "source": [
    "## Problem brakujących wartości"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suitable-bullet",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_train_X = cat_train_X.drop('Cabin', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "static-lincoln",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "num_imputer = SimpleImputer(strategy='median')\n",
    "num_imputer.fit(num_train_X)\n",
    "imputed_num_train_X = num_imputer.transform(num_train_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "consecutive-funds",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train_X = pd.DataFrame(imputed_num_train_X,\n",
    "                              columns=num_train_X.columns,\n",
    "                              index=num_train_X.index)\n",
    "num_train_X.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sized-array",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_imputer = SimpleImputer(strategy='most_frequent')\n",
    "cat_imputer.fit(cat_train_X)\n",
    "imputed_cat_train_X = cat_imputer.transform(cat_train_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "refined-trance",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_train_X = pd.DataFrame(imputed_cat_train_X,\n",
    "                              columns=cat_train_X.columns,\n",
    "                              index=cat_train_X.index)\n",
    "cat_train_X.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fundamental-queens",
   "metadata": {},
   "source": [
    "---\n",
    "<h2><span style=\"color:orange\">Bonus</span></h2>\n",
    "Użyliśmy tradycyjnej imputacji. Poniżej są wykresy przedstawiające obserwacje, które nie zawierają wieku. Widzisz jakieś zależności? Spróbuj dokonać imputacji wielowymiarowej. Poprawia wynik naszego modelu? <br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "headed-appeal",
   "metadata": {},
   "source": [
    "*Tip* `IterativeImputer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hungarian-thursday",
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_age_df = train_data[train_data['Age'].isna()]\n",
    "nan_age_df.hist(figsize=(8,8))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "plain-scratch",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collect-invention",
   "metadata": {},
   "source": [
    "## Zmienne kategoryczne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "athletic-annex",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_train_X['Sex'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "voluntary-memorial",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_train_X['Embarked'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prime-lighting",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_train_X['Ticket'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "perceived-marks",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_train_X = cat_train_X.drop('Ticket', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "representative-detail",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "encoder.fit(cat_train_X)\n",
    "encoder.categories_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "median-colombia",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_train_X = encoder.transform(cat_train_X)\n",
    "cat_train_X = pd.DataFrame(cat_train_X,\n",
    "                              columns=['Female', 'Male', 'C', 'Q', 'S'])\n",
    "cat_train_X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inner-purchase",
   "metadata": {},
   "source": [
    "---\n",
    "<h2><span style=\"color:orange\">Bonus</span></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "clean-promotion",
   "metadata": {},
   "source": [
    "\n",
    "Pominęliśmy być może istotną zmienną `Ticket`. Spróbuj ją zakodować wykorzystując hashing lub kodowanie binarne.\n",
    "Duży plus jeżeli zrobisz to samodzielnie, ale możesz wykorzystać bibliotekę http://contrib.scikit-learn.org/category_encoders/.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "measured-newcastle",
   "metadata": {},
   "source": [
    "## Skalowanie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opposite-deviation",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(num_train_X)\n",
    "scaled_num_train_X = scaler.transform(num_train_X)\n",
    "num_train_X = pd.DataFrame(scaled_num_train_X,\n",
    "                           columns=num_train_X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "competitive-territory",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train_X.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "romantic-heath",
   "metadata": {},
   "source": [
    "## Dyskretyzacja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "greenhouse-convert",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "?pd.cut"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "norwegian-gardening",
   "metadata": {},
   "source": [
    "## Modelowanie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "greenhouse-italic",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = pd.concat([num_train_X, cat_train_X], axis=1)\n",
    "train_X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enormous-enclosure",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "classifier = KNeighborsClassifier()\n",
    "classifier.fit(train_X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "portable-satisfaction",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "train_pred = classifier.predict(train_X)\n",
    "train_scores = cross_val_score(classifier, train_X, y,\n",
    "                               scoring='accuracy', cv=10)\n",
    "np.mean(train_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "separated-curtis",
   "metadata": {},
   "source": [
    "## Preprocessing danych testowych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "immune-prospect",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df, num_imputer, cat_imputer, one_hot_encoder,\n",
    "               scaler):\n",
    "    num_df = df.select_dtypes(exclude=['object'])\n",
    "    cat_df = df.select_dtypes(include=['object'])\n",
    "    #redundancy removal\n",
    "    num_df = num_df.drop('PassengerId', axis=1)\n",
    "    cat_df = cat_df.drop(['Name', 'Cabin'], axis=1)\n",
    "    #handle missing values\n",
    "    imputed_num = num_imputer.transform(num_df) #notice that we do NOT fit\n",
    "    imputed_cat = cat_imputer.transform(cat_df)\n",
    "    num_df = pd.DataFrame(imputed_num,\n",
    "                          columns=num_df.columns,\n",
    "                          index=num_df.index)\n",
    "    cat_df = pd.DataFrame(imputed_cat,\n",
    "                          columns=cat_df.columns,\n",
    "                          index=cat_df.index)\n",
    "    cat_df = cat_df.drop('Ticket', axis=1)\n",
    "    #encode categorical variables\n",
    "    cat_df = encoder.transform(cat_df)\n",
    "    cat_df = pd.DataFrame(cat_df,\n",
    "                          columns=['Female', 'Male', 'C', 'Q', 'S'])\n",
    "    #scaling\n",
    "    scaled_num = scaler.transform(num_df)\n",
    "    num_df = pd.DataFrame(scaled_num,\n",
    "                          columns=num_df.columns)\n",
    "    result_df = pd.concat([num_df, cat_df], axis=1)\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accepting-wagon",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X = preprocess(test_data, num_imputer, cat_imputer, encoder, scaler)\n",
    "test_X.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "little-shopping",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = classifier.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loose-annex",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = np.array([len(train_X) + (i+1) for i in range(len(test_pred))], dtype=int)\n",
    "ids = ids.reshape(-1, 1)\n",
    "test_pred = test_pred.reshape(-1, 1)\n",
    "pred_df = pd.DataFrame(np.concatenate((ids, test_pred), axis=1),\n",
    "                       columns=['PassengerId', 'Survived'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "painful-madrid",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "passive-fleet",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df.to_csv('titanic_predictions.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enhanced-extraction",
   "metadata": {},
   "source": [
    "Gratulacje! Stworzyliśmy pełnoprawny model machine learningu."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amino-personal",
   "metadata": {},
   "source": [
    "---\n",
    "<h2><span style=\"color:orange\">Bonus</span></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daily-emerald",
   "metadata": {},
   "source": [
    "Stworzyliśmy model, ale wykorzystaliśmy domyślne(bardzo przemyślane) hiperparametry, aby go ulepszyć, musimy znaleźć odpowiednie wartości dla naszego problemu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cooperative-chicago",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = [\n",
    "    {\n",
    "     'n_neighbors': [2, 3, 4, 5, 6, 7, 8, 10, 12, 15],\n",
    "     'weights': ['uniform', 'distance'],\n",
    "     'p': [1, 2, 3]}\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "looking-letter",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "grid_search = GridSearchCV(classifier, param_grid, cv=5,\n",
    "                           scoring='accuracy',\n",
    "                           return_train_score=True)\n",
    "grid_search.fit(train_X, y)\n",
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cooperative-housing",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "warming-standard",
   "metadata": {},
   "source": [
    "<h2><span style=\"color:orange\">Bonus II</span></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "magnetic-casting",
   "metadata": {},
   "source": [
    "Preprocessing może być żmudnym procesem. To w jaki sposób przetworzyliśmy dane treningowe, musimy powtórzyć dla danych testowych. Tworzenie dużych i długich funkcji, tak jak `preprocess` może być niewygodne i niesie za sobą ograniczenia.\n",
    "Ponadto, zwróć uwagę, że etapy preprocessingu, również można(nawet trzeba) tuningować poprzez dobór odpowiednich hiperparametrów. W obecnej formie jest to mocno utrudnione. \n",
    "Z pomocą przychodzą pipeline'y:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prerequisite-luther",
   "metadata": {},
   "source": [
    "https://blog.prokulski.science/2020/10/10/pipeline-w-scikit-learn/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "norwegian-miracle",
   "metadata": {},
   "source": [
    "Zapoznaj się z artykułem i spróbuj zbudować prosty pipeline dla danych numerycznych."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
