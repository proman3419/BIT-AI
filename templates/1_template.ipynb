{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"},"colab":{"name":"Notebook.ipynb","provenance":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"dyZmUv2GMc4D"},"source":["# Zajęcia 1 - wstęp do ML, KNN, cross-validation"]},{"cell_type":"markdown","metadata":{"id":"pMoQ-6QCMc4H"},"source":["## Ładowanie danych"]},{"cell_type":"markdown","metadata":{"id":"lBd3XPvWMc4I"},"source":["Na początek zaimportujemy zbiór danych Iris, na którym będziemy pracować. Scikit-learn w Pythonie nazywa się sklearn (bo znak \"-\" nie może być w nazwie modułu).  \n","Sama funkcja load_iris() zwraca obiekt typu Bunch (podobne do słownika), który zawiera sporo danych, m. in.:  \n","- \"data\" - macierz danych X\n","- \"target\" - wektor klas y w postaci liczbowej\n","- \"feature_names\" - nazwy kolejnych cech w kolumnach X\n","- \"target_names\" - nazwy klas z wektora y"]},{"cell_type":"code","metadata":{"id":"cljqNFZAMc4J","executionInfo":{"status":"ok","timestamp":1634406107687,"user_tz":-120,"elapsed":637,"user":{"displayName":"Mykola Haltiuk","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj-HTQlMcjXv5Ignw-w0XJXyYUpITi6vgcq2zhzOg=s64","userId":"08555164971466863527"}}},"source":["from sklearn.datasets import load_iris"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"NGz3LH25Mc4K","outputId":"f3abd28f-2666-462c-b91d-197e0a0861e4"},"source":["dataset = load_iris()\n","feature_names = dataset[\"feature_names\"]\n","target_names = dataset[\"target_names\"]\n","print(\"Features:\", feature_names)\n","print(\"Classes:\", target_names)"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["Features: ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n","Classes: ['setosa' 'versicolor' 'virginica']\n"]}]},{"cell_type":"markdown","metadata":{"id":"s_zaiRNQMc4L"},"source":["Można załadować też tylko same dane numeryczne w postaci macierzy X i wektora y - dla samych algorytmów ML one nas bardziej interesują."]},{"cell_type":"code","metadata":{"id":"oWZ_UfLnMc4L","outputId":"c6d6a611-8268-4a15-9913-6a7fc58c56c2"},"source":["X, y = load_iris(return_X_y=True)\n","print(X)\n","print(y)"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["[[5.1 3.5 1.4 0.2]\n"," [4.9 3.  1.4 0.2]\n"," [4.7 3.2 1.3 0.2]\n"," [4.6 3.1 1.5 0.2]\n"," [5.  3.6 1.4 0.2]\n"," [5.4 3.9 1.7 0.4]\n"," [4.6 3.4 1.4 0.3]\n"," [5.  3.4 1.5 0.2]\n"," [4.4 2.9 1.4 0.2]\n"," [4.9 3.1 1.5 0.1]\n"," [5.4 3.7 1.5 0.2]\n"," [4.8 3.4 1.6 0.2]\n"," [4.8 3.  1.4 0.1]\n"," [4.3 3.  1.1 0.1]\n"," [5.8 4.  1.2 0.2]\n"," [5.7 4.4 1.5 0.4]\n"," [5.4 3.9 1.3 0.4]\n"," [5.1 3.5 1.4 0.3]\n"," [5.7 3.8 1.7 0.3]\n"," [5.1 3.8 1.5 0.3]\n"," [5.4 3.4 1.7 0.2]\n"," [5.1 3.7 1.5 0.4]\n"," [4.6 3.6 1.  0.2]\n"," [5.1 3.3 1.7 0.5]\n"," [4.8 3.4 1.9 0.2]\n"," [5.  3.  1.6 0.2]\n"," [5.  3.4 1.6 0.4]\n"," [5.2 3.5 1.5 0.2]\n"," [5.2 3.4 1.4 0.2]\n"," [4.7 3.2 1.6 0.2]\n"," [4.8 3.1 1.6 0.2]\n"," [5.4 3.4 1.5 0.4]\n"," [5.2 4.1 1.5 0.1]\n"," [5.5 4.2 1.4 0.2]\n"," [4.9 3.1 1.5 0.2]\n"," [5.  3.2 1.2 0.2]\n"," [5.5 3.5 1.3 0.2]\n"," [4.9 3.6 1.4 0.1]\n"," [4.4 3.  1.3 0.2]\n"," [5.1 3.4 1.5 0.2]\n"," [5.  3.5 1.3 0.3]\n"," [4.5 2.3 1.3 0.3]\n"," [4.4 3.2 1.3 0.2]\n"," [5.  3.5 1.6 0.6]\n"," [5.1 3.8 1.9 0.4]\n"," [4.8 3.  1.4 0.3]\n"," [5.1 3.8 1.6 0.2]\n"," [4.6 3.2 1.4 0.2]\n"," [5.3 3.7 1.5 0.2]\n"," [5.  3.3 1.4 0.2]\n"," [7.  3.2 4.7 1.4]\n"," [6.4 3.2 4.5 1.5]\n"," [6.9 3.1 4.9 1.5]\n"," [5.5 2.3 4.  1.3]\n"," [6.5 2.8 4.6 1.5]\n"," [5.7 2.8 4.5 1.3]\n"," [6.3 3.3 4.7 1.6]\n"," [4.9 2.4 3.3 1. ]\n"," [6.6 2.9 4.6 1.3]\n"," [5.2 2.7 3.9 1.4]\n"," [5.  2.  3.5 1. ]\n"," [5.9 3.  4.2 1.5]\n"," [6.  2.2 4.  1. ]\n"," [6.1 2.9 4.7 1.4]\n"," [5.6 2.9 3.6 1.3]\n"," [6.7 3.1 4.4 1.4]\n"," [5.6 3.  4.5 1.5]\n"," [5.8 2.7 4.1 1. ]\n"," [6.2 2.2 4.5 1.5]\n"," [5.6 2.5 3.9 1.1]\n"," [5.9 3.2 4.8 1.8]\n"," [6.1 2.8 4.  1.3]\n"," [6.3 2.5 4.9 1.5]\n"," [6.1 2.8 4.7 1.2]\n"," [6.4 2.9 4.3 1.3]\n"," [6.6 3.  4.4 1.4]\n"," [6.8 2.8 4.8 1.4]\n"," [6.7 3.  5.  1.7]\n"," [6.  2.9 4.5 1.5]\n"," [5.7 2.6 3.5 1. ]\n"," [5.5 2.4 3.8 1.1]\n"," [5.5 2.4 3.7 1. ]\n"," [5.8 2.7 3.9 1.2]\n"," [6.  2.7 5.1 1.6]\n"," [5.4 3.  4.5 1.5]\n"," [6.  3.4 4.5 1.6]\n"," [6.7 3.1 4.7 1.5]\n"," [6.3 2.3 4.4 1.3]\n"," [5.6 3.  4.1 1.3]\n"," [5.5 2.5 4.  1.3]\n"," [5.5 2.6 4.4 1.2]\n"," [6.1 3.  4.6 1.4]\n"," [5.8 2.6 4.  1.2]\n"," [5.  2.3 3.3 1. ]\n"," [5.6 2.7 4.2 1.3]\n"," [5.7 3.  4.2 1.2]\n"," [5.7 2.9 4.2 1.3]\n"," [6.2 2.9 4.3 1.3]\n"," [5.1 2.5 3.  1.1]\n"," [5.7 2.8 4.1 1.3]\n"," [6.3 3.3 6.  2.5]\n"," [5.8 2.7 5.1 1.9]\n"," [7.1 3.  5.9 2.1]\n"," [6.3 2.9 5.6 1.8]\n"," [6.5 3.  5.8 2.2]\n"," [7.6 3.  6.6 2.1]\n"," [4.9 2.5 4.5 1.7]\n"," [7.3 2.9 6.3 1.8]\n"," [6.7 2.5 5.8 1.8]\n"," [7.2 3.6 6.1 2.5]\n"," [6.5 3.2 5.1 2. ]\n"," [6.4 2.7 5.3 1.9]\n"," [6.8 3.  5.5 2.1]\n"," [5.7 2.5 5.  2. ]\n"," [5.8 2.8 5.1 2.4]\n"," [6.4 3.2 5.3 2.3]\n"," [6.5 3.  5.5 1.8]\n"," [7.7 3.8 6.7 2.2]\n"," [7.7 2.6 6.9 2.3]\n"," [6.  2.2 5.  1.5]\n"," [6.9 3.2 5.7 2.3]\n"," [5.6 2.8 4.9 2. ]\n"," [7.7 2.8 6.7 2. ]\n"," [6.3 2.7 4.9 1.8]\n"," [6.7 3.3 5.7 2.1]\n"," [7.2 3.2 6.  1.8]\n"," [6.2 2.8 4.8 1.8]\n"," [6.1 3.  4.9 1.8]\n"," [6.4 2.8 5.6 2.1]\n"," [7.2 3.  5.8 1.6]\n"," [7.4 2.8 6.1 1.9]\n"," [7.9 3.8 6.4 2. ]\n"," [6.4 2.8 5.6 2.2]\n"," [6.3 2.8 5.1 1.5]\n"," [6.1 2.6 5.6 1.4]\n"," [7.7 3.  6.1 2.3]\n"," [6.3 3.4 5.6 2.4]\n"," [6.4 3.1 5.5 1.8]\n"," [6.  3.  4.8 1.8]\n"," [6.9 3.1 5.4 2.1]\n"," [6.7 3.1 5.6 2.4]\n"," [6.9 3.1 5.1 2.3]\n"," [5.8 2.7 5.1 1.9]\n"," [6.8 3.2 5.9 2.3]\n"," [6.7 3.3 5.7 2.5]\n"," [6.7 3.  5.2 2.3]\n"," [6.3 2.5 5.  1.9]\n"," [6.5 3.  5.2 2. ]\n"," [6.2 3.4 5.4 2.3]\n"," [5.9 3.  5.1 1.8]]\n","[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"," 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"," 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n"," 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n"," 2 2]\n"]}]},{"cell_type":"markdown","metadata":{"id":"EPM1_LhnMc4M"},"source":["## Przygotowanie danych"]},{"cell_type":"markdown","metadata":{"id":"09Z7W6TNMc4M"},"source":["Klasy są ułożone po kolei - nie za dobrze, w końcu chcemy trenować i testować nasz klasyfikator na różnych klasach. Do tego trzeba też go podzielić na zbiór treningowy i testowy (walidacyjnego użyjemy później). Na szczęście jest do tego gotowa funkcja train_test_split(), która podzieli nasze dane w losowy sposób.  \n","Jej argumenty:\n","- macierz X\n","- wektor y\n","- test_size - procentowa wielkość zbioru testowego\n","- random_state - seed generatora liczb pseudolosowych, który zarządza "]},{"cell_type":"code","metadata":{"id":"M10SId8yMc4N","outputId":"102851b6-0dd4-41b6-caa2-6027041003ef"},"source":["from sklearn.model_selection import train_test_split\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.25, random_state=0)\n","print(X_train)\n","print(y_train)\n","print()\n","print(X_test)\n","print(y_test)"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["[[5.9 3.  4.2 1.5]\n"," [5.8 2.6 4.  1.2]\n"," [6.8 3.  5.5 2.1]\n"," [4.7 3.2 1.3 0.2]\n"," [6.9 3.1 5.1 2.3]\n"," [5.  3.5 1.6 0.6]\n"," [5.4 3.7 1.5 0.2]\n"," [5.  2.  3.5 1. ]\n"," [6.5 3.  5.5 1.8]\n"," [6.7 3.3 5.7 2.5]\n"," [6.  2.2 5.  1.5]\n"," [6.7 2.5 5.8 1.8]\n"," [5.6 2.5 3.9 1.1]\n"," [7.7 3.  6.1 2.3]\n"," [6.3 3.3 4.7 1.6]\n"," [5.5 2.4 3.8 1.1]\n"," [6.3 2.7 4.9 1.8]\n"," [6.3 2.8 5.1 1.5]\n"," [4.9 2.5 4.5 1.7]\n"," [6.3 2.5 5.  1.9]\n"," [7.  3.2 4.7 1.4]\n"," [6.5 3.  5.2 2. ]\n"," [6.  3.4 4.5 1.6]\n"," [4.8 3.1 1.6 0.2]\n"," [5.8 2.7 5.1 1.9]\n"," [5.6 2.7 4.2 1.3]\n"," [5.6 2.9 3.6 1.3]\n"," [5.5 2.5 4.  1.3]\n"," [6.1 3.  4.6 1.4]\n"," [7.2 3.2 6.  1.8]\n"," [5.3 3.7 1.5 0.2]\n"," [4.3 3.  1.1 0.1]\n"," [6.4 2.7 5.3 1.9]\n"," [5.7 3.  4.2 1.2]\n"," [5.4 3.4 1.7 0.2]\n"," [5.7 4.4 1.5 0.4]\n"," [6.9 3.1 4.9 1.5]\n"," [4.6 3.1 1.5 0.2]\n"," [5.9 3.  5.1 1.8]\n"," [5.1 2.5 3.  1.1]\n"," [4.6 3.4 1.4 0.3]\n"," [6.2 2.2 4.5 1.5]\n"," [7.2 3.6 6.1 2.5]\n"," [5.7 2.9 4.2 1.3]\n"," [4.8 3.  1.4 0.1]\n"," [7.1 3.  5.9 2.1]\n"," [6.9 3.2 5.7 2.3]\n"," [6.5 3.  5.8 2.2]\n"," [6.4 2.8 5.6 2.1]\n"," [5.1 3.8 1.6 0.2]\n"," [4.8 3.4 1.6 0.2]\n"," [6.5 3.2 5.1 2. ]\n"," [6.7 3.3 5.7 2.1]\n"," [4.5 2.3 1.3 0.3]\n"," [6.2 3.4 5.4 2.3]\n"," [4.9 3.  1.4 0.2]\n"," [5.7 2.5 5.  2. ]\n"," [6.9 3.1 5.4 2.1]\n"," [4.4 3.2 1.3 0.2]\n"," [5.  3.6 1.4 0.2]\n"," [7.2 3.  5.8 1.6]\n"," [5.1 3.5 1.4 0.3]\n"," [4.4 3.  1.3 0.2]\n"," [5.4 3.9 1.7 0.4]\n"," [5.5 2.3 4.  1.3]\n"," [6.8 3.2 5.9 2.3]\n"," [7.6 3.  6.6 2.1]\n"," [5.1 3.5 1.4 0.2]\n"," [4.9 3.1 1.5 0.2]\n"," [5.2 3.4 1.4 0.2]\n"," [5.7 2.8 4.5 1.3]\n"," [6.6 3.  4.4 1.4]\n"," [5.  3.2 1.2 0.2]\n"," [5.1 3.3 1.7 0.5]\n"," [6.4 2.9 4.3 1.3]\n"," [5.4 3.4 1.5 0.4]\n"," [7.7 2.6 6.9 2.3]\n"," [4.9 2.4 3.3 1. ]\n"," [7.9 3.8 6.4 2. ]\n"," [6.7 3.1 4.4 1.4]\n"," [5.2 4.1 1.5 0.1]\n"," [6.  3.  4.8 1.8]\n"," [5.8 4.  1.2 0.2]\n"," [7.7 2.8 6.7 2. ]\n"," [5.1 3.8 1.5 0.3]\n"," [4.7 3.2 1.6 0.2]\n"," [7.4 2.8 6.1 1.9]\n"," [5.  3.3 1.4 0.2]\n"," [6.3 3.4 5.6 2.4]\n"," [5.7 2.8 4.1 1.3]\n"," [5.8 2.7 3.9 1.2]\n"," [5.7 2.6 3.5 1. ]\n"," [6.4 3.2 5.3 2.3]\n"," [6.7 3.  5.2 2.3]\n"," [6.3 2.5 4.9 1.5]\n"," [6.7 3.  5.  1.7]\n"," [5.  3.  1.6 0.2]\n"," [5.5 2.4 3.7 1. ]\n"," [6.7 3.1 5.6 2.4]\n"," [5.8 2.7 5.1 1.9]\n"," [5.1 3.4 1.5 0.2]\n"," [6.6 2.9 4.6 1.3]\n"," [5.6 3.  4.1 1.3]\n"," [5.9 3.2 4.8 1.8]\n"," [6.3 2.3 4.4 1.3]\n"," [5.5 3.5 1.3 0.2]\n"," [5.1 3.7 1.5 0.4]\n"," [4.9 3.1 1.5 0.1]\n"," [6.3 2.9 5.6 1.8]\n"," [5.8 2.7 4.1 1. ]\n"," [7.7 3.8 6.7 2.2]\n"," [4.6 3.2 1.4 0.2]]\n","[1 1 2 0 2 0 0 1 2 2 2 2 1 2 1 1 2 2 2 2 1 2 1 0 2 1 1 1 1 2 0 0 2 1 0 0 1\n"," 0 2 1 0 1 2 1 0 2 2 2 2 0 0 2 2 0 2 0 2 2 0 0 2 0 0 0 1 2 2 0 0 0 1 1 0 0\n"," 1 0 2 1 2 1 0 2 0 2 0 0 2 0 2 1 1 1 2 2 1 1 0 1 2 2 0 1 1 1 1 0 0 0 2 1 2\n"," 0]\n","\n","[[5.8 2.8 5.1 2.4]\n"," [6.  2.2 4.  1. ]\n"," [5.5 4.2 1.4 0.2]\n"," [7.3 2.9 6.3 1.8]\n"," [5.  3.4 1.5 0.2]\n"," [6.3 3.3 6.  2.5]\n"," [5.  3.5 1.3 0.3]\n"," [6.7 3.1 4.7 1.5]\n"," [6.8 2.8 4.8 1.4]\n"," [6.1 2.8 4.  1.3]\n"," [6.1 2.6 5.6 1.4]\n"," [6.4 3.2 4.5 1.5]\n"," [6.1 2.8 4.7 1.2]\n"," [6.5 2.8 4.6 1.5]\n"," [6.1 2.9 4.7 1.4]\n"," [4.9 3.6 1.4 0.1]\n"," [6.  2.9 4.5 1.5]\n"," [5.5 2.6 4.4 1.2]\n"," [4.8 3.  1.4 0.3]\n"," [5.4 3.9 1.3 0.4]\n"," [5.6 2.8 4.9 2. ]\n"," [5.6 3.  4.5 1.5]\n"," [4.8 3.4 1.9 0.2]\n"," [4.4 2.9 1.4 0.2]\n"," [6.2 2.8 4.8 1.8]\n"," [4.6 3.6 1.  0.2]\n"," [5.1 3.8 1.9 0.4]\n"," [6.2 2.9 4.3 1.3]\n"," [5.  2.3 3.3 1. ]\n"," [5.  3.4 1.6 0.4]\n"," [6.4 3.1 5.5 1.8]\n"," [5.4 3.  4.5 1.5]\n"," [5.2 3.5 1.5 0.2]\n"," [6.1 3.  4.9 1.8]\n"," [6.4 2.8 5.6 2.2]\n"," [5.2 2.7 3.9 1.4]\n"," [5.7 3.8 1.7 0.3]\n"," [6.  2.7 5.1 1.6]]\n","[2 1 0 2 0 2 0 1 1 1 2 1 1 1 1 0 1 1 0 0 2 1 0 0 2 0 0 1 1 0 2 1 0 2 2 1 0\n"," 1]\n"]}]},{"cell_type":"markdown","metadata":{"id":"POdq8s2zMc4N"},"source":["## Wykorzystanie kNN ze Scikit-learn"]},{"cell_type":"markdown","metadata":{"id":"IWmxfgjRMc4O"},"source":["Wykorzystamy teraz gotowy klasyfikator ze Scikit-learn i zapoznamy się z typowym API tych klasyfikatorów.  \n","\n","Sam klasyfikator to obiekt pewnej klasy. Tworząc ten obiekt, ustawiamy mu wartości hiperparametrów.  \n","\n","Kolejnym krokiem jest wywołanie metody trenującej fit(X, y), w której przekazujemy dane i wartości, z których następnie uczy się klasyfikator.  \n","\n","Później można już dokonywać klasyfikacji, metodą predict(X), która przyjmuje macierz danych i zwraca wektor klas, które przewiduje dla kolejnych przykładów (wierszy z X).  \n","  \n","Inne przydatne metody (nie wszystkie są we wszystkich klasyfikatorach):  \n","- predict_proba(X) - zwraca prawdopodobieństwa poszczególnych klas, a nie pojedynczą predykcję; dla kNN jest to rozkład klas wśród sąsiadów\n","- score(X, y) - zwraca accuracy na przekazanych danych testowych (tutaj X i y to zbiór testowy!)\n","- get_params() - zwraca parametry klasyfikatora"]},{"cell_type":"code","metadata":{"id":"a81tX5HsMc4O"},"source":["from sklearn.neighbors import KNeighborsClassifier"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IeGctaKaMc4O","outputId":"1621f7e4-5338-4f2d-9ee7-481f05bd8283"},"source":["model = KNeighborsClassifier()\n","model.fit(X_train, y_train)"],"execution_count":null,"outputs":[{"data":{"text/plain":["KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n","                     metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n","                     weights='uniform')"]},"execution_count":47,"metadata":{},"output_type":"execute_result"}]},{"cell_type":"markdown","metadata":{"id":"K_xZl7lhMc4P"},"source":["Powyżej można zaobserwować różne parametry dotyczące ściśle kNN. Przyjęły one wartości domyślne, bo nic nie przekazaliśmy w konstruktorze.  \n","Najważniejsze:\n","- n_neighbors - po prostu k\n","- algorithm - sposób obliczania sąsiadów, \"auto\" wybiera albo k-d tree, albo ball tree (w zależności od tego, co wyceni jako korzystniejsze na podstawie rozmiaru danych)  \n","- metric - wykorzystywana metryka; jeżeli wymaga dodatkowych parametrów (np. metryka Mahalanobisa wymaga odwrotnej macierzy kowariancji), to są one przekazywane w metric_params; musi być to obiekt klasy DistanceMetric lub coś implementującego jej interfejs\n","- weights - schemat ważenia sąsiadów, może być to zdefiniowana przez nas funkcja z odpowiednim interfejsem  \n","\n","Teraz dokonamy faktycznej predykcji danych testowych i porównamy je z prawdziwymi. Najpierw \"skleimy\" wektory do macierzy 2D, traktując je jak kolumny, a potem je zwizualizujemy. Wykorzystamy tutaj biblioteki:\n","- Numpy - standard do obliczeń numerycznych, zapewnia operacje na wektorach i macierzach; prawie każda biblioteka ML (włącznie ze Scikit-learn) korzysta z niego \"pod spodem\"\n","- Pandas - biblioteka do manipulacji danymi, pozwoli ładnie wyświetlić dane z opisem konkretnych kolumn (używa ramki danych DataFrame, coś jak macierz 2D, ale wierszy i kolumn można używać jak słowników)."]},{"cell_type":"code","metadata":{"id":"POhEjgvQMc4P"},"source":["import numpy as np\n","import pandas as pd"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"v7GyS1VTMc4Q","outputId":"256b11ed-83df-4c95-b12d-49277f165e9e"},"source":["y_true = y_test\n","y_pred = model.predict(X_test)\n","ys = np.column_stack((y_true, y_pred))  # double parentheses, because argument has to be a tuple\n","dataframe = pd.DataFrame(data=ys, columns=[\"y_true\", \"y_pred\"])\n","print(dataframe)"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["    y_true  y_pred\n","0        2       2\n","1        1       1\n","2        0       0\n","3        2       2\n","4        0       0\n","5        2       2\n","6        0       0\n","7        1       1\n","8        1       1\n","9        1       1\n","10       2       2\n","11       1       1\n","12       1       1\n","13       1       1\n","14       1       1\n","15       0       0\n","16       1       1\n","17       1       1\n","18       0       0\n","19       0       0\n","20       2       2\n","21       1       1\n","22       0       0\n","23       0       0\n","24       2       2\n","25       0       0\n","26       0       0\n","27       1       1\n","28       1       1\n","29       0       0\n","30       2       2\n","31       1       1\n","32       0       0\n","33       2       2\n","34       2       2\n","35       1       1\n","36       0       0\n","37       1       2\n"]}]},{"cell_type":"markdown","metadata":{"id":"xzae6lYuMc4Q"},"source":["Z samego takiego wypisania danych niewiele wynika - spróbujmy zatem określić jakość klasyfikatora pojedynczą liczbą.  "]},{"cell_type":"markdown","metadata":{"id":"GozQzSOnMc4Q"},"source":["### Zadanie 1"]},{"cell_type":"markdown","metadata":{"id":"x0BFLThGMc4Q"},"source":["Napisz funkcję, która obliczy najprostszy sposób mierzenia jakości klasyfikatora - **celność**. Jest to stosunek liczby poprawnie zaklasyfikowanych przykładów do liczby wszystkich przykładów. Jaka jest precyzja kNN z domyślnymi parametrami dla zbioru Iris?"]},{"cell_type":"code","metadata":{"id":"ZFNZR542Mc4R"},"source":["def accuracy(y_pred, y_true):\n","    pass  # implement me!"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iKxZIYS8Mc4R"},"source":["### Zadanie 2"]},{"cell_type":"markdown","metadata":{"id":"kMo3gUOXMc4R"},"source":["Tym razem wykorzystaj zbiór Breast Cancer Wisconsin (także jest dostępny ze Scikit-learn'a) - jest trochę trudniejszy od zbioru Iris i będzie lepiej na nim widać pewne zależności. Przeczytaj o nim na stronie UCI Machine Learning Repository - jest to główne źródło zbiorów danych w ML, w Scikit-learnie są tylko te najpopularniejsze i to bez szczegółowych opisów.  \n","\n","Kod, na którym można się wzorować: https://machinelearningmastery.com/implement-resampling-methods-scratch-python/.\n","\n","Może ci pomóc kod korzystający w większej mierze z rozwiązań ze Sciki-learn'a z następnej części notebooka (patrz niżej). Z Matplotlibem może pomóc kod z https://kevinzakka.github.io/2016/07/13/k-nearest-neighbor/ oraz https://scriptverse.academy/tutorials/python-matplotlib-plot-straight-line.html.\n","\n","Po załadowaniu go wykonaj \"ręcznie\" walidację skrośną dla różnych wartości k (liczba sąsiadów kNN):  \n","1. Idź pętlą for dla kilku różnych wartości k, np. 1, 3, 5 i sqrt(n)  \n","2. Dla każdej wartości k idź zagnieżdżoną pętlą for tak, aby \"wycinać\" po 20% zbioru treningowego jako zbiór walidacyjny (5-fold cross validation), tzn. dla n przykładów najpierw przykłady o indeksach od 0 do 0.2n, potem od 0.2n+1 do 0.4n itd. (trzeba to zaokrąglić do liczb całkowitych); resztę łącz w zbiór treningowy. Przyda się tutaj slicing dla tablic Numpy'a.  \n","3. Dla każdego foldu oblicz celność klasyfikatora kNN (dla tego k, które w danej chwili jest w zewnętrznej pętli).  \n","4. Dla każdej wartości k żywając biblioteki Matplotlib wykonaj prosty wykres (punkty + łączące je linie) celności dla poszczególnych foldów. Dorysuj do niego także poziomą linię prostą ze średnią celnością.\n","\n","Jak dużą wariancję celności (jej zmienność na wykresie) miał klasyfikator dla różnych k? Czy uśrednienie celności dla każdej wartości k (to robi walidacja skrośna) ma sens, sądząc po rysunku (czy daje faktyczne informacje o osiągach klasyfikatora)?"]},{"cell_type":"code","metadata":{"id":"8XENAz_IMc4S"},"source":["# uncomment things as you implement them\n","# import things from sklearn\n","# dataset = ...\n","\n","# for k in ...\n","    # create folds here \n","#    for fold in folds:\n","        # create training set, train kNN, calculate precision using your function on the fold data\n","    # plot"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qGEEdIBOMc4S"},"source":["## Walidacja skrośna, metryki pomiaru jakości klasyfikatorów"]},{"cell_type":"markdown","metadata":{"id":"DpnJxcO7Mc4S"},"source":["Wykorzystamy teraz walidację skrośną ze Scikit-learn'a, aby automatycznie zyskać najlepszą wartość parametru k dla zbioru danych. Użyjemy zbioru Breast Cancer Wisconsin, który powinieneś/powinnaś był/a załadować w zadaniu 2. Sprawdzając różne k, wybierzemy takie, które daje największą średnią celność (używając zaimplementowanej przez ciebie funkcji) na zbiorach walidacyjnych. Na koniec sprawdzimy, konkretnie jak dużą precyzję daje to wybrane k.  \n","\n","Uwaga: używamy tutaj funkcji cross_val_score, która jednocześnie przeprowadza walidację skrośną i zwraca uśredniony wynik wybranego sposobu pomiaru jakości klasyfikatora. Jest także dostępna np. funkcja KFold, która po prostu generuje kolejne foldy, po których można przechodzić pętlą for."]},{"cell_type":"code","metadata":{"id":"0dSY01UFMc4T","outputId":"4dfd2192-91d9-4ce0-a085-5701306007cd"},"source":["from sklearn.datasets import load_breast_cancer  # load dataset\n","from sklearn.metrics import accuracy_score  # automatic accuracy calculation\n","from sklearn.model_selection import cross_val_score  # cross validation with scores calculation\n","\n","def get_best_k(X_train, y_train):\n","    best_k = -1\n","    best_score = -1\n","    X_rows_num = X.shape[0]  # .shape[0] for 2D matrix is it's rows number (number of samples), .shape[1] is number of columns (features)\n","    for k in range(1, int(np.sqrt(X_rows_num))):\n","        model = KNeighborsClassifier(n_neighbors=k)\n","        # cv=5 is the default value, we could omit it\n","        # error_score='raise' is here to suppress warnings\n","        scores = cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy', error_score='raise')\n","        mean_score = scores.mean()\n","        if mean_score > best_score:\n","            best_k = k\n","            best_score = mean_score\n","    return best_k\n","\n","\n","X, y = load_breast_cancer(return_X_y=True)\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.25, random_state=0)\n","\n","best_k = get_best_k(X_train, y_train)\n","print(\"Best k:\", best_k)\n","\n","model = KNeighborsClassifier(n_neighbors=best_k)\n","model.fit(X_train, y_train)\n","y_pred = model.predict(X_test)\n","accuracy = accuracy_score(y_test, y_pred)\n","print(\"Accuracy:\", round(accuracy, 3))"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["Best k: 10\n","Accuracy: 0.944\n"]}]},{"cell_type":"markdown","metadata":{"id":"Ut8osoqCMc4T"},"source":["### Zadanie 3"]},{"cell_type":"markdown","metadata":{"id":"CX1F5BTFMc4U"},"source":["Na podstawie funkcji get_best_k stwórz funkcję, która będzie wizualizować wyniki dla różnych k na wykresie. Przetestuj ją dla zbioru Breast Cancer Wisconsin - najlepsza wartość k z wykresu powinna być taka sama, jak obliczona wyżej."]},{"cell_type":"code","metadata":{"id":"yR4c422CMc4U"},"source":["def plot_accuracy_for_k_values(X_train, y_train):\n","    pass  # implement me!"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6KRGjDZ7Mc4U"},"source":["### Zadanie 4"]},{"cell_type":"markdown","metadata":{"id":"QMxcDYlrMc4U"},"source":["Rozszerz funkcję get_best_k o argument scoring, który będzie napisem wskazującym na to, co chcemy mierzyć za pomocą walidacji skrośnej. Przetestuj, czy ta sama wartość k jest optymalna dla accuracy, precision, recall i F1."]},{"cell_type":"code","metadata":{"id":"2Hkd8yvWMc4U"},"source":["# implement me!"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7-t5-EthMc4U"},"source":["## Zbadanie klątwy wymiaru"]},{"cell_type":"markdown","metadata":{"id":"j-ygN75JMc4U"},"source":["Uwaga: poniższe ćwiczenie ze względu na dziwną strukturę zbioru jest dość czasochłonne. Ilustruje ważne zagadnienie, ale jest mniej ważne, niż samodzielna implementacja klasyfikatora kNN, która jest w następnym ćwiczeniu. Warto jednak to wykonać, jeżeli ma się czas, żeby zobaczyć, na czym dokładnie polega klątwa wymiaru i jak można ją zminimalizować.\n","\n","Na dysku Google ściągnij zbiór \"multiDimHypercubes.mat\". Za pomocą biblioteki Scipy i poniższej funkcji wczytaj go (musi być w tym samym katalogu, co notebook), a następnie zapoznaj się z jego strukturą (jest dość nietypowa).\n","\n","Sprawdź błąd klasyfikacji (1 - celność) metody 1-NN dla serii problemów klasyfikacyjnych w zależności od wymiarowości przestrzeni (liczby cech i). Stwórz wykres zależności błędu od wymiarowości przestrzeni i.\n","\n","Oblicz, dla każdego i, średnią z odległości każdego przykładu testowego do najbliższego przykładu treningowego z danej klasy oraz do przykładu treningowego z klasy przeciwnej, oraz stosunek tych średnich, po czym przedstaw je na wykresiew zależności od i.\n","\n","O czym to świadczy? Czy zmiana metryki na inną, wspieraną przez KNeighborsClassifier (patrz klasa DistanceMetric), np. Manhattan lub Mahalanobisa (to jest trochę bardziej tricky, trzeba obliczyć macierz kowariancji, odwrócić ją i przekazać jako argument), poprawia to?  \n","Uwaga: jeżeli dla metryki Mahalanobisa jakiegoś zbioru macierz kowariancji jest osobliwa (singular), to należy złapać błąd przez try/except i pominąć ten zbiór."]},{"cell_type":"code","metadata":{"id":"AKaB9IqTMc4V"},"source":["from scipy.io import loadmat\n","\n","def load_hypercubes():\n","    data = loadmat(\"hypercubes.mat\")\n","    X_train = data['featuresTrain'][0]\n","    X_test = data['featuresTest'][0]\n","    y_train = data['classesTrain'][0]\n","    y_test = data['classesTest'][0]\n","    max_dimensions = data['maxDim'][0][0]\n","    return X_train, X_test, y_train, y_test, max_dimensions"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rfh1FIDRMc4V"},"source":["# write code!"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"t4Tt5gZfMc4V"},"source":["## Samodzielna implementacja klasyfikatora kNN"]},{"cell_type":"markdown","metadata":{"id":"tyoyltD2Mc4V"},"source":["Napisz klasę KNNClassifier, która:\n","- w konstruktorze będzie przyjmować argument k o wartości domyślnej 1\n","- będzie posiadała metodę .fit(X, y), w której będzie tworzyć k-d tree (użyj implementacji ze Scikit-learn) na podstawie danych; zastanów się, co zrobić z tym, że KDTree trzyma tylko wektory X, a nie wartości klas (podpowiedź: co zwraca metoda .query(X, k) dla tej klasy?)\n","- będzie posiadała metodę .predict(X), która dokona predykcji dla przekazanej macierzy punktów"]},{"cell_type":"code","metadata":{"id":"gLCdeemYMc4V"},"source":["class KNNClassifier:\n","    pass # implement me!"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3PWj1AGuMc4W"},"source":["Proponowane możliwe ulepszenia:\n","- możliwość ustawienia wartości k \"auto\", która w metodzie .fit() sprawdzi wybrane wartości k (np. 1, 3, 5, ..., sqrt(n)) i wybierze optymalną\n","- przekazywanie w konstruktorze argumentu \"metric\" z wartością domyślną \"euclidean\", żeby wykorzystywać alternatywną metrykę\n","- zaimplementować wybrany sposób lub sposoby ważenia sąsiadów, najprostsze jest chyba z odwrotnością odległości lub z szeregiem harmonicznym; sprawdzić, czy zwiększa precyzję (uwaga: optymalne k może być inne, niż dla domyślnego ważenia \"uniform\"!)\n","\n","Nie trzeba szczególnie się skupiać na obsłudze błędów etc., nie o to tu chodzi (chyba, że się wam chce, to wtedy czemu nie)."]}]}